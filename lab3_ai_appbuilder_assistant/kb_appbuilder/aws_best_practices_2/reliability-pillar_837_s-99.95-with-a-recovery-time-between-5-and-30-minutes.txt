{"page_content": "3\u00bd 9s (99.95%) with a Recovery Time between 5 and 30 Minutes - Reliability Pillar3\u00bd 9s (99.95%) with a Recovery Time between 5 and 30 Minutes - Reliability PillarAWSDocumentationAWS Well-ArchitectedAWS Well-Architected FrameworkMonitor resourcesAdapt to changes in demandImplement changeBack up dataArchitect for resiliencyTest resiliencyPlan for disaster recovery (DR)Availability design goal3\u00bd 9s (99.95%) with a Recovery Time between 5 and 30       Minutes\n        This availability goal for applications requires extremely short\n        downtime and very little data loss during specific times.\n        Applications with this availability goal include applications in\n        the areas of: banking, investing, emergency services, and data\n        capture. These applications have very short recovery times and\n        recovery points.\n      \n        We can improve recovery time further by using a Warm\n        Standby approach across two AWS Regions. We will\n        deploy the entire workload to both Regions, with our passive\n        site scaled down and all data kept eventually consistent. Both\n        deployments will be statically stable\n        within their respective regions. The applications should be\n        built using the distributed system resiliency patterns. We\u2019ll\n        need to create a lightweight routing\n        component that monitors for workload health, and can be\n        configured to route traffic to the passive region if necessary.\n      \nMonitor resources\n\n          There will be alerting on every replacement of a web server,\n          when the database fails over, and when the Region fails over.\n          We will also monitor the static content on Amazon S3 for\n          availability and alert if it becomes unavailable. Logging will\n          be aggregated for ease of management and to help in root cause\n          analysis in each Region.\n        \n\n          The routing component monitors both our application health and\n          any regional hard dependencies we have.\n        \nAdapt to changes in demand\n\n          Same as the 4 9s scenario.\n        \nImplement change\n\n          Delivery of new software is on a fixed schedule of every two\n          to four weeks. Software updates will be automated using canary\n          or blue/green deployment patterns.\n        \n\n          Runbooks exist for when Region failover occurs, for common\n          customer issues that occur during those events, and for common\n          reporting.\n        \n\n          We will have playbooks for common database problems,\n          security-related incidents, failed deployments, unexpected\n          customer issues on Region failover, and establishing root\n          cause of problems. After the root cause has been identified,\n          the correction of error will be identified by a combination of\n          the operations and development teams and deployed when the fix\n          is developed.\n        \n\n          We will also engage with AWS Support for Infrastructure Event\n          Management.\n        \nBack up data\n\n          Like the 4 9s scenario, we use automatic RDS backups and use S3\n          versioning. Data is automatically and asynchronously\n          replicated from the Aurora RDS cluster in the active region to\n          cross-region read replicas in the passive region. S3\n          cross-region replication is used to automatically and\n          asynchronously move data from the active to the passive\n          region.\n        \nArchitect for resiliency\n\n          Same as the 4 9s scenario, plus regional failover is possible.\n          This is managed manually. During failover, we will route\n          requests to a static website using DNS failover until recovery\n          in the second Region.\n        \nTest resiliency\n\n          Same as the 4 9s scenario plus we will validate the\n          architecture through game days using runbooks. Also RCA\n          correction is prioritized above feature releases for immediate\n          implementation and deployment\n        \nPlan for disaster recovery (DR)\n Regional failover is manually managed. All data is asynchronously replicated.\n          Infrastructure in the warm standby is scaled out. This can be\n          automated using a workflow invoked on AWS Step Functions. AWS Systems Manager (SSM) can also help with\n          this automation, as you can create SSM documents that update Auto Scaling groups and\n          resize instances. \nAvailability design goal\n\n          We assume that at least some failures will require a manual\n          decision to run recovery, however with good automation in\n          this scenario we assume that only two events per year will\n          require this decision. We take 20 minutes to decide to run\n          recovery, and assume that recovery is completed within 10\n          minutes. This implies that it takes 30 minutes to recover from\n          failure. Assuming two failures per year, our estimated impact\n          time for the year is 60 minutes.\n        \n\n          This means that the upper limit on availability is 99.95%. The\n          actual availability will also depend on the real rate of\n          failure, the duration of the failure, and how quickly each\n          failure actually recovers. For this architecture, we assume\n          that the application is online continuously through updates.\n          Based on this, our availability design\n          goal is 99.95%.\n        \n\nSummary\n\n\n\n\nTopic\n\n\nImplementation\n\n\n\n\n\n                  Monitor resources\n                \n\n                  Health checks at all layers, including DNS health at\n                  AWS Region level, and on KPIs; alerts sent when\n                  configured alarms are tripped; alerting on all\n                  failures. Operational meetings are rigorous to detect\n                  trends and manage to design goals.\n                \n\n\n\n                  Adapt to changes in demand\n                \n ELB for web and automatic scaling application tier; automatic scaling\n                  storage and read replicas in multiple zones in the active and passive regions for\n                  Aurora RDS. Data and infrastructure synchronized between AWS Regions for static\n                  stability. \n\n\n\n                  Implement change\n                \n\n                  Automated deploy via canary or blue/green and\n                  automated rollback when KPIs or alerts indicate\n                  undetected problems in application, deployments are\n                  made to one isolation zone in one AWS Region at a\n                  time.\n                \n\n\n\n                  Back up data\n                \n\n                  Automated backups in each AWS Region via RDS to meet\n                  RPO and automated restoration that is practiced\n                  regularly in a game day. Aurora RDS and S3 data is\n                  automatically and asynchronously replicated from\n                  active to passive region.\n                \n\n\n\n                  Architect for resiliency\n                \n\n                  Automatic scaling to provide self-healing web and\n                  application tier; RDS is Multi-AZ; regional failover\n                  is managed manually with static site presented while\n                  failing over.\n                \n\n\n\n                  Test resiliency\n                \n\n                  Component and isolation zone fault testing is in\n                  pipeline and practiced with operational staff\n                  regularly in a game day; playbooks exist for\n                  diagnosing unknown problems; and a Root Cause Analysis\n                  process exists, with communication paths for what the\n                  problem was, and how it was corrected or prevented.\n                  RCA correction is prioritized above feature releases\n                  for immediate implementation and deployment.\n                \n\n\n\n                  Plan for disaster recovery (DR)\n                \n Warm Standby deployed in another region. Infrastructure is scaled out using\n                  workflows invoked using AWS Step Functions or AWS Systems Manager Documents. Encrypted backups via\n                  RDS. Cross-region read replicas between two AWS Regions. Cross-region replication\n                  of static assets in Amazon S3. Restoration is to the current active AWS Region, is\n                  practiced in a game day, and is coordinated with AWS. \n\n\n\n\n Javascript is disabled or is unavailable in your browser.To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.Document ConventionsMulti-Region scenarios 5 9s (99.999%) or higher scenario with a recovery time under one minuteDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.", "metadata": {"source": "https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/s-99.95-with-a-recovery-time-between-5-and-30-minutes.html", "title": "3\u00bd 9s (99.95%) with a Recovery Time between 5 and 30 Minutes - Reliability Pillar", "description": "This availability goal for applications requires extremely short downtime and very little data loss during specific times. Applications with this availability goal include applications in the areas of: banking, investing, emergency services, and data capture. These applications have very short recovery times and recovery points.", "language": "en-US"}}