{"page_content": "4 9s (99.99%) scenario - Reliability Pillar4 9s (99.99%) scenario - Reliability PillarAWSDocumentationAWS Well-ArchitectedAWS Well-Architected FrameworkMonitor resourcesAdapt to changes in demandImplement changeBack up dataArchitect for resiliencyTest resiliencyPlan for disaster recovery (DR)Availability design goalSummary4 9s (99.99%) scenario\n        This availability goal for applications requires the application\n        to be highly available and tolerant to component failures. The\n        application must be able to absorb failures without needing to\n        get additional resources. This availability goal is for mission\n        critical applications that are main or significant revenue\n        drivers for a corporation, such as an ecommerce site, a business\n        to business web service, or a high traffic content/media site.\n       We can improve availability further by using an architecture that will be\n          statically stable within the Region. This availability goal doesn\u2019t\n        require a control plane change in behavior of our workload to tolerate failure. For example,\n        there should be enough capacity to withstand the loss of one Availability Zone. We should\n        not require updates to Amazon Route\u00a053 DNS. We should not need to create any new infrastructure,\n        whether it\u2019s creating or modifying an S3 bucket, creating new IAM policies (or\n        modifications of policies), or modifying Amazon ECS task configurations. \nMonitor resources\n\n          Monitoring will include success metrics as well as alerting\n          when problems occur. In addition, there will be alerting on\n          every replacement of a failed web server, when the database\n          fails over, and when an AZ fails.\n        \nAdapt to changes in demand\n We will use Amazon Aurora as our RDS, which allows automatic scaling of read replicas.\n          For these applications, engineering for read availability over write availability of\n          primary content is also a key architecture decision. Aurora can also automatically grow\n          storage as needed, in 10 GB increments up to 128 TB.\nImplement change\n\n          We will deploy updates using canary or blue/green deployments\n          into each isolation zone separately. The deployments are fully\n          automated, including a roll back if KPIs indicate a problem.\n        \n\n          Runbooks will exist for rigorous reporting requirements and\n          performance tracking. If successful operations are trending\n          toward failure to meet performance or availability goals, a\n          playbook will be used to establish what is causing the trend.\n          Playbooks will exist for undiscovered failure modes and\n          security incidents. Playbooks will also exist for establishing\n          the root cause of failures. We will also engage with AWS Support for Infrastructure Event Management offering.\n        \n\n          The team that builds and operates the website will identify\n          the correction of error of any unexpected failure and\n          prioritize the fix to be deployed after it is implemented.\n        \nBack up data\n\n          Backup and restore can be done using Amazon RDS. It will be\n          run regularly using a runbook to ensure that we can meet\n          recovery requirements.\n        \nArchitect for resiliency\n\n          We recommend three Availability Zones for this approach. Using\n          a three Availability Zone deployment, each AZ has static\n          capacity of 50% of peak. Two Availability Zones could be used,\n          but the cost of the statically stable capacity would be more\n          because both zones would have to have 100% of peak capacity.\n          We will add Amazon CloudFront to provide geographic caching,\n          as well as request reduction on our application\u2019s data plane.\n        \n We will use Amazon Aurora as our RDS and deploy read replicas in all three zones. \n\n          The application will be built using the software/application\n          resiliency patterns in all layers.\n        \nTest resiliency\n\n          The deployment pipeline will have a full test suite, including\n          performance, load, and failure injection\n          testing.\n\n\n          We will practice our failure recovery procedures constantly\n          through game days, using runbooks to ensure that we can\n          perform the tasks and not deviate from the procedures. The\n          team that builds the website also operates the website.\n        \nPlan for disaster recovery (DR)\n\n          Runbooks exist for total workload recovery and common\n          reporting. Recovery uses backups stored in the same region as\n          the workload. Restore procedures are regularly exercised as\n          part of game days.\n        \nAvailability design goal\n\n          We assume that at least some failures will require a manual\n          decision to perform recovery, however with greater automation\n          in this scenario we assume that only two events per year will\n          require this decision and the recovery actions will be rapid.\n          We take 10 minutes to decide to run recovery, and assume\n          that recovery is completed within five minutes. This implies\n          15 minutes to recover from failure. Assuming two failures per\n          year, our estimated impact time for the year is 30 minutes.\n        \n\n          This means that the upper limit on availability is 99.99%. The\n          actual availability will also depend on the real rate of\n          failure, the duration of the failure, and how quickly each\n          failure actually recovers. For this architecture, we assume\n          that the application is online continuously through updates.\n          Based on this, our availability design\n          goal is 99.99%.\n        \nSummary\n\n\n\nTopic\n\n\nImplementation\n\n\n\n\n\n                  Monitor resources\n                \n\n                  Health checks at all layers and on KPIs; alerts sent\n                  when configured alarms are tripped; alerting on all\n                  failures. Operational meetings are rigorous to detect\n                  trends and manage to design goals.\n                \n\n\n\n                  Adapt to changes in demand\n                \n ELB for web and automatic scaling application tier; automatic scaling\n                  storage and read replicas in multiple zones for Aurora RDS. \n\n\n\n                  Implement change\n                \n\n                  Automated deploy via canary or blue/green and\n                  automated rollback when KPIs or alerts indicate\n                  undetected problems in application. Deployments are\n                  made by isolation zone.\n                \n\n\n\n                  Back up data\n                \n\n                  Automated backups via RDS to meet RPO and automated\n                  restoration that is practiced regularly in a game day.\n                \n\n\n\n                  Architect for resiliency\n                \n\n                  Implemented fault isolation zones for the application;\n                  auto scaling to provide self-healing web and\n                  application tier; RDS is Multi-AZ.\n                \n\n\n\n                  Test resiliency\n                \n\n                  Component and isolation zone fault testing is in\n                  pipeline and practiced with operational staff\n                  regularly in a game day; playbooks exist for\n                  diagnosing unknown problems; and a Root Cause Analysis\n                  process exists.\n                \n\n\n\n                  Plan for disaster recovery (DR)\n                \n\n                  Encrypted backups via RDS to same AWS Region that is\n                  practiced in a game day.\n                \n\n\n Javascript is disabled or is unavailable in your browser.To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.Document Conventions3 9s (99.9%) scenarioMulti-Region scenarios Did this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.", "metadata": {"source": "https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/s-99.99-scenario.html", "title": "4 9s (99.99%) scenario - Reliability Pillar", "description": "This availability goal for applications requires the application to be highly available and tolerant to component failures. The application must be able to absorb failures without needing to get additional resources. This availability goal is for mission critical applications that are main or significant revenue drivers for a corporation, such as an ecommerce site, a business to business web service, or a high traffic content/media site.", "language": "en-US"}}