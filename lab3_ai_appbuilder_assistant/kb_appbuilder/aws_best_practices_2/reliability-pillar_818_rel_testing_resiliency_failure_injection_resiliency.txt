{"page_content": "REL12-BP05 Test resiliency using chaos engineering - Reliability PillarREL12-BP05 Test resiliency using chaos engineering - Reliability PillarAWSDocumentationAWS Well-ArchitectedAWS Well-Architected FrameworkImplementation guidanceImplementation stepsResourcesREL12-BP05 Test resiliency using chaos engineering\n    Run chaos experiments regularly in environments that are in or as close to production as possible to understand how your system responds to adverse conditions. \n  \n\n      Desired outcome:\n    \n\n    The resilience of the workload is regularly verified by applying chaos engineering in the form of fault injection experiments or injection of unexpected load, \n    in addition to resilience testing that validates known expected behavior of your workload during an event. Combine both chaos engineering and resilience testing \n    to gain confidence that your workload can survive component failure and can recover from unexpected disruptions with minimal to no impact.\n  \n\n      Common anti-patterns:\n    \n\n\n Designing for resiliency, but not verifying how the workload functions as a whole when\n        faults occur. \n\n\n       Never experimenting under real-world conditions and expected load.\n     \n\n\n       Not treating your experiments as code or maintaining them through the development cycle.\n     \n\n\n       Not running chaos experiments both as part of your CI/CD pipeline, as well as outside of deployments.\n     \n\n\n       Neglecting to use past post-incident analyses when determining which faults to experiment with.\n     \n\n Benefits of establishing this best practice: Injecting faults to \n    verify the resilience of your workload allows you to gain confidence that the recovery procedures of \n    your resilient design will work in the case of a real fault.\n  \nLevel of risk exposed if this best practice is not established: Medium\n  \nImplementation guidance\n\n      Chaos engineering provides your teams with capabilities to continually inject real world \n      disruptions (simulations) in a controlled way at the service provider, infrastructure, workload, \n      and component level, with minimal to no impact to your customers. It allows your teams to learn \n      from faults and observe, measure, and improve the resilience of your workloads, as well as validate \n      that alerts fire and teams get notified in the case of an event. \n    \n\n      When performed continually, chaos engineering can highlight deficiencies in your workloads that, \n      if left unaddressed, could negatively affect availability and operation.\n    \nNoteChaos engineering is the discipline of experimenting on a system in order to build\n      confidence in the system\u2019s capability to withstand turbulent conditions in production. \u2013\n        Principles of Chaos Engineering\n\n\n    If a system is able to withstand these disruptions, the chaos experiment should be maintained \n    as an automated regression test. In this way, chaos experiments should be performed as part of \n    your systems development lifecycle (SDLC) and as part of your CI/CD pipeline.\n  \n\n    To ensure that your workload can survive component failure, inject real world events as part of \n    your experiments. For example, experiment with the loss of Amazon EC2 instances or failover of the primary \n    Amazon RDS database instance, and verify that your workload is not impacted (or only minimally impacted). \n    Use a combination of component faults to simulate events that may be caused by a disruption in an Availability Zone.\n  \n\n    For application-level faults (such as crashes), you can start with stressors such as memory and CPU exhaustion.\n  \n \n      To validate fallback or failover mechanisms \n      for external dependencies due to intermittent network disruptions, your components should simulate such an event by blocking access to the\n      third-party providers for a specified duration that can last from seconds to hours. \n\n    Other modes of degradation might cause reduced functionality and slow responses, often resulting in a disruption of your services. \n    Common sources of this degradation are increased latency on critical services and unreliable network communication (dropped packets). \n    Experiments with these faults, including networking effects such as latency, dropped messages, and DNS failures, could include the \n    inability to resolve a name, reach the DNS service, or establish connections to dependent services.\n  \n\nChaos engineering tools:\n\n\n    AWS Fault Injection Service (AWS FIS) is a fully managed service for running fault injection \n    experiments that can be used as part of your CD pipeline, or outside of the pipeline. AWS FIS is a \n    good choice to use during chaos engineering game days. It supports simultaneously introducing \n    faults across different types of resources including Amazon EC2, Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Kubernetes Service (Amazon EKS), and Amazon RDS. \n    These faults include termination of resources, forcing failovers, stressing CPU or memory, throttling, \n    latency, and packet loss. Since it is integrated with Amazon CloudWatch Alarms, you can set up stop \n    conditions as guardrails to rollback an experiment if it causes unexpected impact.  \n  \n\n\nAWS Fault Injection Service integrates with AWS\n          resources to allow you to run fault injection experiments for your\n        workloads.\n\nThere are also several third-party options for fault injection experiments. These include\n      open-source tools such as Chaos Toolkit, Chaos Mesh, and Litmus Chaos, as well as commercial options like Gremlin. To expand the scope of\n      faults that can be injected on AWS, AWS FIS integrates with Chaos Mesh and Litmus Chaos, allowing you to coordinate fault\n      injection workflows among multiple tools. For example, you can run a stress test on a pod\u2019s\n      CPU using Chaos Mesh or Litmus faults while terminating a randomly selected percentage of\n      cluster nodes using AWS FIS fault actions.\n  \nImplementation steps\n\n\n\n    Determine which faults to use for experiments. \n    \n Assess the design of your workload for resiliency. Such designs (created using the best\n          practices of the Well-Architected Framework) account for risks based on critical dependencies,\n          past events, known issues, and compliance requirements. List each element of the design\n          intended to maintain resilience and the faults it is designed to mitigate. For more\n          information about creating such lists, see the Operational Readiness Review whitepaper which guides you on how to\n          create a process to prevent reoccurrence of previous incidents. The Failure Modes and\n          Effects Analysis (FMEA) process provides you with a framework for performing a component-level\n          analysis of failures and how they impact your workload. FMEA is outlined in more detail by\n          Adrian Cockcroft in Failure Modes and Continuous Resilience. \n\n\n          Assign a priority to each fault.\n        \n\n          Start with a coarse categorization such as high, medium, or low. \n          To assess priority, consider frequency of the fault and impact of failure to the overall workload.\n        \n\n          When considering frequency of a given fault, analyze past data for this workload when available. \n          If not available, use data from other workloads running in a similar environment. \n        \n\n          When considering impact of a given fault, the larger the scope of the fault, generally the larger the impact. \n          Also consider the workload design and purpose. For example, the ability to access the source data stores is \n          critical for a workload doing data transformation and analysis. In this case, you would prioritize experiments \n          for access faults, as well as throttled access and latency insertion.\n        \n\n          Post-incident analyses are a good source of data to understand both frequency and impact of failure modes. \n        \n\n          Use the assigned priority to determine which faults to experiment with first \n          and the order with which to develop new fault injection experiments. \n        \n\n\n          For each experiment that you perform, follow the chaos engineering and continuous resilience flywheel in the following figure.\n        \n\n\nChaos engineering and continuous resilience flywheel, using the scientific method by Adrian Hornsby.\n\n \n\n\n\n              Define steady state as some measurable output of a workload that indicates normal behavior.\n            \n\n              Your workload exhibits steady state if it is operating reliably and as expected. Therefore, validate that your \n              workload is healthy before defining steady state. Steady state does not necessarily mean no impact to the workload \n              when a fault occurs, as a certain percentage in faults could be within acceptable limits. The steady state is \n              your baseline that you will observe during the experiment, which will highlight anomalies if your hypothesis \n              defined in the next step does not turn out as expected.\n            \n\n              For example, a steady state of a payments system can be defined as the processing of 300 TPS with a success rate \n              of 99% and round-trip time of 500 ms. \n            \n\n\n              Form a hypothesis about how the workload will react to the fault.\n            \n\n              A good hypothesis is based on how the workload is expected to mitigate the fault to maintain the steady state. \n              The hypothesis states that given the fault of a specific type, the system or workload will continue steady state, \n              because the workload was designed with specific mitigations. The specific type of fault and mitigations should be \n              specified in the hypothesis.", "metadata": {"source": "https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/rel_testing_resiliency_failure_injection_resiliency.html", "title": "REL12-BP05 Test resiliency using chaos engineering - Reliability Pillar", "description": "Run chaos experiments regularly in environments that are in or as close to production as possible to understand how your system responds to adverse conditions.", "language": "en-US"}}