{"page_content": "The following template can be used for the hypothesis (but other wording is also acceptable):\n            \nNote\n                If specific fault occurs, the workload name workload will describe mitigating controls to maintain\n                business or technical metric impact.\n              \n\n              For example:\n            \n\n\n\n                  If 20% of the nodes in the Amazon EKS node-group are taken down, the Transaction Create API continues to serve the 99th percentile of \n                  requests in under 100 ms (steady state). The Amazon EKS nodes will recover within five minutes, and pods will get scheduled and process \n                  traffic within eight minutes after the initiation of the experiment. Alerts will fire within three minutes.\n                \n\n\n                  If a single Amazon EC2 instance failure occurs, the order system\u2019s Elastic Load Balancing health check will cause the Elastic Load Balancing \n                  to only send requests to the remaining healthy instances while the Amazon EC2 Auto Scaling replaces the failed instance, maintaining a \n                  less than 0.01% increase in server-side (5xx) errors (steady state).\n                \n\n\n                  If the primary Amazon RDS database instance fails, the Supply Chain data collection workload will failover and connect to the standby \n                  Amazon RDS database instance to maintain less than 1 minute of database read or write errors (steady state).\n                \n\n\n\n              Run the experiment by injecting the fault.\n            \n An experiment should by default be fail-safe and tolerated by the workload. If\n              you know that the workload will fail, do not run the experiment. Chaos engineering\n              should be used to find known-unknowns or unknown-unknowns. Known-unknowns are things you are aware of but don\u2019t fully understand,\n              and unknown-unknowns are things you are neither\n              aware of nor fully understand. Experimenting against a workload that you know is\n              broken won\u2019t provide you with new insights. Your experiment should be carefully\n              planned, have a clear scope of impact, and provide a rollback mechanism that can be\n              applied in case of unexpected turbulence. If your due-diligence shows that your\n              workload should survive the experiment, move forward with the\n              experiment. There are several options for injecting the faults. For workloads on\n              AWS, AWS FIS provides many predefined fault simulations called actions. You\n              can also define custom actions that run in AWS FIS using AWS Systems Manager\n                documents. \n\n              We discourage the use of custom scripts for chaos experiments, unless the scripts \n              have the capabilities to understand the current state of the workload, are able to emit logs, \n              and provide mechanisms for rollbacks and stop conditions where possible.\n            \n An effective framework or toolset which supports chaos engineering should track\n              the current state of an experiment, emit logs, and provide rollback mechanisms to\n              support the controlled running of an experiment. Start with an established service\n              like AWS FIS that allows you to perform experiments with a clearly defined scope and\n              safety mechanisms that rollback the experiment if the experiment introduces unexpected\n              turbulence. To learn about a wider variety of experiments using AWS FIS, also see the\n                Resilient and Well-Architected Apps with Chaos Engineering lab. Also,\n                AWS Resilience Hub will\n              analyze your workload and create experiments that you can choose to implement and run\n              in AWS FIS. \n            \nNote\n                For every experiment, clearly understand the scope and its impact. We recommend that \n                faults should be simulated first on a non-production environment before being run in production.\n              \n \n              Experiments should run in production under real-world load using \n              canary deployments \n              that spin up both a control and experimental system deployment, where feasible. Running experiments during off-peak times is a good\n              practice to mitigate potential impact when first experimenting in production. Also, if\n              using actual customer traffic poses too much risk, you can run experiments using\n              synthetic traffic on production infrastructure against the control and experimental\n              deployments. When using production is not possible, run experiments in pre-production\n              environments that are as close to production as possible. \n            \n You must establish and monitor guardrails to ensure the experiment does not\n              impact production traffic or other systems beyond acceptable limits. Establish stop\n              conditions to stop an experiment if it reaches a threshold on a guardrail metric that\n              you define. This should include the metrics for steady state for the workload, as well\n              as the metric against the components into which you\u2019re injecting the fault. A synthetic monitor \n              (also known as a user canary) is one metric you should\n              usually include as a user proxy. Stop conditions for AWS FIS \n              are supported as part of the experiment template, allowing up to five stop-conditions per template. \n            \n\n              One of the principles of chaos is minimize the scope of the experiment and its impact:\n            \n\n              While there must be an allowance for some short-term negative impact, it is the responsibility and \n              obligation of the Chaos Engineer to ensure the fallout from experiments are minimized and contained.\n            \n\n              A method to verify the scope and potential impact is to perform the experiment in a non-production environment \n              first, verifying that thresholds for stop conditions activate as expected during an experiment and observability \n              is in place to catch an exception, instead of directly experimenting in production.\n            \n\n              When running fault injection experiments, verify that all responsible parties are well-informed. Communicate with \n              appropriate teams such as the operations teams, service reliability teams, and customer support to let them know \n              when experiments will be run and what to expect. Give these teams communication tools to inform those running the \n              experiment if they see any adverse effects.\n            \n\n              You must restore the workload and its underlying systems back to the original known-good state. Often, the resilient \n              design of the workload will self-heal. But some fault designs or failed experiments can leave your workload in an \n              unexpected failed state. By the end of the experiment, you must be aware of this and restore the workload and systems. \n              With AWS FIS you can set a rollback configuration (also called a post action) within the action parameters. A post action \n              returns the target to the state that it was in before the action was run. Whether automated (such as using AWS FIS) or \n              manual, these post actions should be part of a playbook that describes how to detect and handle failures.\n            \n\n\n              Verify the hypothesis.\n            \nPrinciples of Chaos\n                Engineering gives this guidance on how to verify steady state of your\n              workload: \nFocus on the measurable output of a system, rather than internal attributes of the system. \n              Measurements of that output over a short period of time constitute a proxy for the system\u2019s steady state. \n              The overall system\u2019s throughput, error rates, and latency percentiles could all be metrics of interest \n              representing steady state behavior. By focusing on systemic behavior patterns during experiments, chaos engineering verifies \n              that the system does work, rather than trying to validate how it works.\n\n              In our two previous examples, we include the steady state metrics of less than 0.01% increase in server-side (5xx) errors and \n              less than one minute of database read and write errors.\n            \n\n              The 5xx errors are a good metric because they are a consequence of the failure mode that a client of the workload will experience directly. \n              The database errors measurement is good as a direct consequence of the fault, but should also be supplemented with a client impact measurement \n              such as failed customer requests or errors surfaced to the client. Additionally, include a synthetic monitor (also known as a user canary) on \n              any APIs or URIs directly accessed by the client of your workload.\n            \n\n\n              Improve the workload design for resilience.\n            \n If steady state was not maintained, then investigate how the workload design can\n              be improved to mitigate the fault, applying the best practices of the AWS Well-Architected\n                Reliability pillar. Additional guidance and resources can be found in the\n                AWS Builder\u2019s Library,\n              which hosts articles about how to improve your health\n                checks or employ retries with\n                backoff in your application code, among others.", "metadata": {"source": "https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/rel_testing_resiliency_failure_injection_resiliency.html", "title": "REL12-BP05 Test resiliency using chaos engineering - Reliability Pillar", "description": "Run chaos experiments regularly in environments that are in or as close to production as possible to understand how your system responds to adverse conditions.", "language": "en-US"}}