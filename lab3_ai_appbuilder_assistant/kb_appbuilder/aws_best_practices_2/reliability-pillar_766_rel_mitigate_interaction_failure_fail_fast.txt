{"page_content": "REL05-BP04 Fail fast and limit queues - Reliability PillarREL05-BP04 Fail fast and limit queues - Reliability PillarAWSDocumentationAWS Well-ArchitectedAWS Well-Architected FrameworkImplementation guidanceImplementation stepsResourcesREL05-BP04 Fail fast and limit queuesWhen a service is unable to respond successfully to a request, fail fast. This allows resources associated with a request to be released, and permits a service to recover if it\u2019s running out of resources. Failing fast is a well-established software design pattern that can be leveraged to build highly reliable workloads in the cloud. Queuing is also a well-established enterprise integration pattern that can smooth load and allow clients to release resources when asynchronous processing can be tolerated. When a service is able to respond successfully under normal conditions but fails when the rate of requests is too high, use a queue to buffer requests. However, do not allow a buildup of long queue backlogs that can result in processing stale requests that a client has already given up on.\nDesired outcome: When systems experience resource contention, timeouts, exceptions, or grey failures that make service level objectives unachievable, fail fast strategies allow for faster system recovery. Systems that must absorb traffic spikes and can accommodate asynchronous processing can improve reliability by allowing clients to quickly release requests by using queues to buffer requests to backend services. When buffering requests to queues, queue management strategies are implemented to avoid insurmountable backlogs. \n  \nCommon anti-patterns:\n\n\n\n        Implementing message queues but not configuring dead letter queues (DLQ) or alarms on DLQ volumes to detect when a system is in failure.\n      \n\n\n        Not measuring the age of messages in a queue, a measurement of latency to understand when queue consumers are falling behind or erroring out causing retrying.\n      \n\n\n        Not clearing backlogged messages from a queue, when there is no value in processing these messages if the business need no longer exists.\n      \n\n\n        Configuring first in first out (FIFO) queues when last in first out (LIFO) queues would better serve client needs, for example when strict ordering is not required and backlog processing is delaying all new and time sensitive requests resulting in all clients experiencing breached service levels.\n      \n\n\n        Exposing internal queues to clients instead of exposing APIs that manage work intake and place requests into internal queues.\n      \n\n\n        Combining too many work request types into a single queue which can exacerbate backlog conditions by spreading resource demand across request types.\n      \n\n\n        Processing complex and simple requests in the same queue, despite needing different monitoring, timeouts and resource allocations.\n      \n\n\n        Not validating inputs or using assertions to implement fail fast mechanisms in software that bubble up exceptions to higher level components that can handle errors gracefully.\n      \n\n\n        Not removing faulty resources from request routing, especially when failures are grey emitting both successes and failures due to crashing and restarting, intermittent dependency failure, reduced capacity, or network packet loss.\n      \n\nBenefits of establishing this best practice: Systems that fail fast are easier to debug and fix, and often expose issues in coding and configuration before releases are published into production. Systems that incorporate effective queueing strategies provide greater resilience and reliability to traffic spikes and intermittent system fault conditions. \n  \nLevel of risk exposed if this best practice\n    is not established: High\n  \nImplementation guidance\n\n      Fail fast strategies can be coded into software solutions as well as configured into infrastructure. In addition to failing fast, queues are a straightforward yet powerful architectural technique to decouple system components smooth load. Amazon CloudWatch provides capabilities to monitor for and alarm on failures. Once a system is known to be failing, mitigation strategies can be invoked, including failing away from impaired resources. When systems implement queues with Amazon SQS and other queue technologies to smooth load, they must consider how to manage queue backlogs, as well as message consumption failures. \n    \nImplementation steps\n\n\n\n          Implement programmatic assertions or specific metrics in your software and use them to explicitly alert on system issues. Amazon CloudWatch helps you create metrics and alarms based on application log pattern and SDK instrumentation.\n        \n\n\n          Use CloudWatch metrics and alarms to fail away from impaired resources that are adding latency to processing or repeatedly failing to process requests. \n        \n\n\n          Use asynchronous processing by designing APIs to accept requests and append requests to internal queues using Amazon SQS and then respond to the message-producing client with a success message so the client can release resources and move on with other work while backend queue consumers process requests.\n        \n\n\n          Measure and monitor for queue processing latency by producing a CloudWatch metric each time you take a message off a queue by comparing now to message timestamp.\n        \n\n\n          When failures prevent successful message processing or traffic spikes in volumes that cannot be processed within service level agreements, sideline older or excess traffic to a spillover queue. This allows priority processing of new work, and older work when capacity is available. This technique is an approximation of LIFO processing and allows normal system processing for all new work. \n        \n\n\n          Use dead letter or redrive queues to move messages that can\u2019t be processed out of the backlog into a location that can be researched and resolved later\n        \n\n\n          Either retry or, when tolerable, drop old messages by comparing now to the message timestamp and discarding messages that are no longer relevant to the requesting client.\n        \n\nResources\n\nRelated best practices:\n\n\n\n\nREL04-BP02 Implement loosely coupled dependencies\n\n\n\nREL05-BP02 Throttle requests\n\n\n\nREL05-BP03 Control and limit retry calls\n\n\n\nREL06-BP02 Define and calculate metrics (Aggregation)\n\n\n\nREL06-BP07 Monitor end-to-end tracing of requests through your\n  system\n\n\n\nRelated documents:\n\n\n\n\n          Avoiding insurmountable queue backlogs\n        \n\n\nFail\n          Fast\n\n\n\n          How can I prevent an increasing backlog of messages in my Amazon SQS queue?\n        \n\n\n          Elastic Load Balancing: Zonal Shift\n        \n\n\n          Amazon Route\u00a053 Application Recovery Controller: Routing control for traffic failover\n        \n\n\nRelated examples:\n\n\n\n\n          Enterprise Integration Patterns: Dead Letter Channel\n        \n\n\nRelated videos:\n\n\n\n\nAWS re:Invent 2022 - Operating highly available Multi-AZ applications\n\n\n\nRelated tools:\n\n\n\n\n          Amazon SQS\n        \n\n\n          Amazon MQ\n        \n\nAWS IoT Core\n\n\n          Amazon CloudWatch\n        \n\n Javascript is disabled or is unavailable in your browser.To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.Document ConventionsREL05-BP03 Control and limit retry callsREL05-BP05 Set client timeoutsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.", "metadata": {"source": "https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/rel_mitigate_interaction_failure_fail_fast.html", "title": "REL05-BP04 Fail fast and limit queues - Reliability Pillar", "description": "When a service is unable to respond successfully to a request, fail fast. This allows resources associated with a request to be released, and permits a service to recover if it\u2019s running out of resources. Failing fast is a well-established software design pattern that can be leveraged to build highly reliable workloads in the cloud. Queuing is also a well-established enterprise integration pattern that can smooth load and allow clients to release resources when asynchronous processing can be tolerated. When a service is able to respond successfully under normal conditions but fails when the rate of requests is too high, use a queue to buffer requests. However, do not allow a buildup of long queue backlogs that can result in processing stale requests that a client has already given up on.", "language": "en-US"}}