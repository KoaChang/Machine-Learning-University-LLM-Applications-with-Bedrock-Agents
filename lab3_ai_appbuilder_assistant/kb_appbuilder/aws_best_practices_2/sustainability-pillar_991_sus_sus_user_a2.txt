{"page_content": "SUS02-BP01 Scale workload infrastructure dynamically - Sustainability PillarSUS02-BP01 Scale workload infrastructure dynamically - Sustainability PillarAWSDocumentationAWS Well-ArchitectedAWS Well-Architected FrameworkImplementation guidanceImplementation stepsResourcesSUS02-BP01 Scale workload infrastructure dynamicallyUse elasticity of the cloud and scale your infrastructure dynamically to match supply of cloud \n    resources to demand and avoid overprovisioned capacity in your workload.Common anti-patterns:\nYou do not scale your infrastructure with user load.You manually scale your infrastructure all the time.You leave increased capacity after a scaling event instead of scaling back down.\nBenefits of establishing this best practice: Configuring and testing \n    workload elasticity help to efficiently match supply of cloud resources to demand and avoid overprovisioned \n    capacity. You can take advantage of elasticity in the cloud to automatically scale capacity during and \n    after demand spikes to make sure you are only using the right number of resources needed to meet your \n    business requirements.\nLevel of risk exposed if this best practice\n    is not established: Medium\n  \nImplementation guidance\n\n  The cloud provides the flexibility to expand or reduce your resources dynamically through a variety of \n  mechanisms to meet changes in demand. Optimally matching supply to demand delivers the lowest environmental \n  impact for a workload.  \n\n\n      Demand can be fixed or variable, requiring metrics and automation to make sure that management does \n      not become burdensome. Applications can scale vertically (up or down) by modifying the instance size, \n      horizontally (in or out) by modifying the number of instances, or a combination of both.\n    \n\n      You can use a number of different approaches to match supply of resources with demand. \n    \n\n\n\nTarget-tracking approach: Monitor your scaling metric and \n          automatically increase or decrease capacity as you need it.\n        \n\n\nPredictive scaling: Scale in anticipation of daily and weekly trends.\n        \n\n\nSchedule-based approach: Set your own scaling schedule according to \n          predictable load changes.\n        \n\n\nService scaling: Pick services (like serverless) that are natively \n          scaling by design or provide auto scaling as a feature.\n        \n\n\n        Identify periods of low or no utilization and scale resources to remove excess capacity and improve efficiency.\n      \nImplementation steps\n\n\nElasticity matches the supply of resources you have against the demand for those resources. \n          Instances, containers, and functions provide mechanisms for elasticity, either in combination with \n          automatic scaling or as a feature of the service. AWS provides a range of auto scaling mechanisms \n          to ensure that workloads can scale down quickly and easily during periods of low user load. Here \n          are some examples of auto scaling mechanisms:\n\n\nAuto scaling mechanism\nWhere to use\n\n\n\n\nAmazon EC2 Auto Scaling\n\n\nUse to verify you have the correct number of Amazon EC2 instances available to \n                    handle the user load for your application.\n                  \n\n\n\n\nApplication Auto Scaling\n\n\nUse to automatically scale the resources for individual AWS services beyond Amazon EC2, \n                    such as Lambda functions or Amazon Elastic Container Service (Amazon ECS) services.\n                  \n\n\n\n\n\nKubernetes Cluster Autoscaler\n\n\n\nUse to automatically scale Kubernetes clusters on AWS.\n\n\n\n\n\n          Scaling is often discussed related to compute services like Amazon EC2 instances or AWS Lambda \n          functions. Consider the configuration of non-compute services like Amazon DynamoDB read and write \n          capacity units or Amazon Kinesis Data Streams shards to match the demand.\n        \n\n\n          Verify that the metrics for scaling up or down are validated against the type of workload being deployed. \n          If you are deploying a video transcoding application, 100% CPU utilization is expected and should not be your primary metric. \n          You can use a customized metric (such as memory utilization) for your \n          scaling policy if required. To choose the right metrics, consider the following guidance for Amazon EC2:\n        \n\n\n\n              The metric should be a valid utilization metric and describe how busy an instance is.\n            \n\n\n              The metric value must increase or decrease proportionally to the number of instances in the Auto Scaling group.\n            \n\n\n\n          Use dynamic scaling instead of \n          manual scaling for your Auto Scaling group. \n          We also recommend that you use target \n            tracking scaling policies in your dynamic scaling. \n        \n\n\n           Verify that workload deployments can handle both scale-out and scale-in events. Create test scenarios for scale-in events to verify that the workload behaves as expected\n           and does not affect the user experience (like losing sticky sessions). You can use \n          Activity history to verify a scaling activity for an Auto Scaling group.\n        \n\n\n          Evaluate your workload for predictable patterns and proactively scale as you anticipate predicted and planned changes in demand. With predictive scaling, you can eliminate the need to overprovision capacity. For more detail, see \n          Predictive Scaling with Amazon EC2 Auto Scaling.\n        \n\nResources\n\nRelated documents:\n\n\n\n\nGetting\n          Started with Amazon EC2 Auto Scaling\n\n\n\nPredictive\n          Scaling for EC2, Powered by Machine Learning\n\n\n\nAnalyze\n          user behavior using Amazon OpenSearch Service, Amazon Data Firehose and Kibana\n\n\n\nWhat\n          is Amazon CloudWatch?\n\n\n\nMonitoring\n          DB load with Performance Insights on Amazon RDS\n\n\n\nIntroducing Native Support for Predictive Scaling with Amazon EC2 Auto Scaling\n\n\n\nIntroducing Karpenter - An Open-Source, High-Performance Kubernetes Cluster Autoscaler\n\n\n\nDeep Dive on Amazon ECS Cluster Auto Scaling\n\n\n\nRelated videos:\n\n\n\n\nBuild a cost-, energy-, and resource-efficient\n          compute environment\n\n\n\nBetter,\n          faster, cheaper compute: Cost-optimizing Amazon EC2\n          (CMP202-R1)\n\n\n\nRelated examples:\n\n\n\n\nLab: Amazon EC2 Auto Scaling Group Examples\n\n\n\nLab: Implement Autoscaling with Karpenter\n\n\n Javascript is disabled or is unavailable in your browser.To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.Document ConventionsAlignment to demandSUS02-BP02 Align SLAs with sustainability goalsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.", "metadata": {"source": "https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/sus_sus_user_a2.html", "title": "SUS02-BP01 Scale workload infrastructure dynamically - Sustainability Pillar", "description": "Use elasticity of the cloud and scale your infrastructure dynamically to match supply of cloud resources to demand and avoid overprovisioned capacity in your workload.", "language": "en-US"}}