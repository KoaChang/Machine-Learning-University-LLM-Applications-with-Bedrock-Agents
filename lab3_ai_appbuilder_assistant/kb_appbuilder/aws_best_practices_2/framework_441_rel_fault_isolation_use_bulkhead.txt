{"page_content": "REL10-BP04 Use bulkhead architectures to limit scope of impact - AWS Well-Architected FrameworkREL10-BP04 Use bulkhead architectures to limit scope of impact - AWS Well-Architected FrameworkAWSDocumentationAWS Well-ArchitectedFrameworkImplementation guidanceResourcesREL10-BP04 Use bulkhead architectures to limit scope of\n  impactImplement bulkhead architectures (also known as cell-based architectures) to restrict the effect of failure within a workload to a limited number of components.\nDesired outcome: A cell-based architecture uses multiple isolated instances of a workload, where each instance is known as a cell. Each cell is independent, does not share state with other cells, and handles a subset of the overall workload requests. This reduces the potential impact of a failure, such as a bad software update, to an individual cell and the requests it is processing. If a workload uses 10 cells to service 100 requests, when a failure occurs, 90% of the overall requests would be unaffected by the failure.\n  \nCommon anti-patterns:\n\n\n\n        Allowing cells to grow without bounds.\n      \n\n\n        Applying code updates or deployments to all cells at the same time.\n      \n\n\n        Sharing state or components between cells (with the exception of the router layer).\n      \n\n\n        Adding complex business or routing logic to the router layer.\n      \n\n\n        Not minimizing cross-cell interactions.\n      \n\nBenefits of establishing this best practice: With cell-based architectures, many common types of failure are contained within the cell itself, providing additional fault isolation. These fault boundaries can provide resilience against failure types that otherwise are hard to contain, such as unsuccessful code deployments or requests that are corrupted or invoke a specific failure mode (also known as poison pill requests).\n  \nLevel of risk exposed if this best practice\n      is not established: High\n  \nImplementation guidance\n\n      On a ship, bulkheads ensure that a hull breach is contained within one section of the hull. In complex systems, this pattern is often replicated to allow fault isolation. Fault isolated boundaries restrict the effect of a failure within a workload to a limited number of components. Components outside of the boundary are unaffected by the failure. Using multiple fault isolated boundaries, you can limit the impact on your workload. On AWS, customers can use multiple Availability Zones and Regions to provide fault isolation, but the concept of fault isolation can be extended to your workload\u2019s architecture as well.\n    \n\n      The overall workload is partitioned cells by a partition key. This key needs to align with the grain of the service, or the natural way that a service's workload can be subdivided with minimal cross-cell interactions. Examples of partition keys are customer ID, resource ID, or any other parameter easily accessible in most API calls. A cell routing layer distributes requests to individual cells based on the partition key and presents a single endpoint to clients.\n    \n\n\nFigure 11: Cell-based architecture\n\n\nImplementation steps\n\n\n      When designing a cell-based architecture, there are several design considerations to consider:\n    \n\n\n\nPartition key: Special consideration should be taken\n          while choosing the partition key. \n\n\n\n              It should align with the grain of the service, or the natural way that a service's workload can be subdivided with minimal cross-cell interactions. Examples are customer ID or resource ID.\n            \n\n\n              The partition key must be available in all requests, either directly or in a way that could be easily inferred deterministically by other parameters.\n            \n\n\n\nPersistent cell mapping: Upstream services should only\n          interact with a single cell for the lifecycle of their resources. \n\n\n\n              Depending on the workload, a cell migration strategy may be needed to migrate data from one cell to another. A possible scenario when a cell migration may be needed is if a particular user or resource in your workload becomes too big and requires it to have a dedicated cell.\n            \n\n\n              Cells should not share state or components between cells.\n            \n\n\n              Consequently, cross-cell interactions should be avoided or kept to a minimum, as those interactions create dependencies between cells and therefore diminish the fault isolation improvements.\n            \n\n\n\nRouter layer: The router layer is a shared component\n          between cells, and therefore cannot follow the same compartmentalization strategy as with\n          cells. \n\n\n\n              It is recommended for the router layer to distribute requests to individual cells using a partition mapping algorithm in a computationally efficient manner, such as combining cryptographic hash functions and modular arithmetic to map partition keys to cells.\n            \n\n\n              To avoid multi-cell impacts, the routing layer must remain as simple and horizontally scalable as possible, which necessitates avoiding complex business logic within this layer. This has the added benefit of making it easy to understand its expected behavior at all times, allowing for thorough testability. As explained by Colm MacC\u00e1rthaigh in Reliability, constant work, and a good cup of coffee, simple designs and constant work patterns produce reliable systems and reduce anti-fragility.\n            \n\n\n\nCell size: Cells should have a maximum size and should\n          not be allowed to grow beyond it. \n\n\n\n              The maximum size should be identified by performing thorough testing, until breaking points are reached and safe operating margins are established. For more detail on how to implement testing practices, see REL07-BP04 Load test your workload\n\n\n\n              The overall workload should grow by adding additional cells, allowing the workload to scale with increases in demand.\n            \n\n\n\nMulti-AZ or Multi-Region strategies: Multiple layers of\n          resilience should be leveraged to protect against different failure domains. \n\n\n\n              For resilience, you should use an approach that builds layers of defense. One layer protects against smaller, more common disruptions by building a highly available architecture using multiple AZs. Another layer of defense is meant to protect against rare events like widespread natural disasters and Region-level disruptions. This second layer involves architecting your application to span multiple AWS Regions. Implementing a multi-Region strategy for your workload helps protect it against widespread natural disasters that affect a large geographic region of a country, or technical failures of Region-wide scope. Be aware that implementing a multi-Region architecture can be significantly complex, and is usually not required for most workloads. For more detail, see REL10-BP02 Select the appropriate locations for your\n  multi-location deployment.\n            \n\n\n\nCode deployment: A staggered code deployment strategy\n          should be preferred over deploying code changes to all cells at the same time. \n\n\n\n              This helps minimize potential failure to multiple cells due to a bad deployment or human error. For more detail, see Automating safe, hands-off deployment.\n            \n\n\nResources\n\nRelated best practices:\n\n\n\n\nREL07-BP04 Load test your workload\n\n\n\nREL10-BP02 Select the appropriate locations for your\n  multi-location deployment\n\n\n\nRelated documents:\n\n\n\n\nReliability, constant work, and a good cup of coffee\n\n\nAWS and Compartmentalization\n        \n\n\n          Workload isolation using shuffle-sharding\n        \n\n\nAutomating safe, hands-off deployment\n\n\n\nRelated videos:\n\n\n\nAWS re:Invent 2018: Close Loops and Opening Minds: How to Take Control of Systems, Big and Small\n        \n\n\nAWS re:Invent\n          2018: How AWS Minimizes the Blast Radius of Failures\n          (ARC338)\n\n\n\nShuffle-sharding:\n          AWS re:Invent 2019: Introducing The Amazon Builders\u2019 Library\n          (DOP328)\n\n\nAWS Summit ANZ 2021 - Everything fails, all the time: Designing for resilience \n        \n\n\nRelated examples:\n\n\n\n\nWell-Architected\n          Lab - Fault isolation with shuffle sharding\n\n\n Javascript is disabled or is unavailable in your browser.To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.Document ConventionsREL10-BP03 Automate recovery for components constrained to a\n  single locationREL 11. How do you design your workload to withstand component\n                  failures?Did this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.", "metadata": {"source": "https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_fault_isolation_use_bulkhead.html", "title": "REL10-BP04 Use bulkhead architectures to limit scope of impact - AWS Well-Architected Framework", "description": "Implement bulkhead architectures (also known as cell-based architectures) to restrict the effect of failure within a workload to a limited number of components.", "language": "en-US"}}