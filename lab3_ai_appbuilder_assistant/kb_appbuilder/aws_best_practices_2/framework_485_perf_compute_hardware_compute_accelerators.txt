{"page_content": "PERF02-BP06 Use optimized hardware-based compute accelerators - AWS Well-Architected FrameworkPERF02-BP06 Use optimized hardware-based compute accelerators - AWS Well-Architected FrameworkAWSDocumentationAWS Well-ArchitectedFrameworkImplementation guidanceResourcesPERF02-BP06 Use optimized hardware-based compute\n  accelerators\n    Use hardware accelerators to perform certain functions more\n    efficiently than CPU-based alternatives.\n  \nCommon anti-patterns:\n\n\n\n        In your workload, you haven't benchmarked a general-purpose\n        instance against a purpose-built instance that can\n        deliver higher performance and lower cost.\n      \n\n\n        You are using hardware-based compute accelerators for tasks that\n        can be more efficient using CPU-based alternatives.\n      \n\n\n        You are not monitoring GPU usage.\n      \nBenefits of establishing this best practice: By using hardware-based accelerators, such as graphics processing units (GPUs) and field programmable gate arrays (FPGAs), you can perform certain processing functions more efficiently.\n  \nLevel of risk exposed if this best practice\n    is not established: Medium\n  \nImplementation guidance\n\n      Accelerated computing instances provide access to hardware-based\n      compute accelerators such as GPUs and FPGAs. These hardware\n      accelerators perform certain functions like graphic processing or\n      data pattern matching more efficiently than CPU-based\n      alternatives. Many accelerated workloads, such as rendering,\n      transcoding, and machine learning, are highly variable in terms of\n      resource usage. Only run this hardware for the time needed, and\n      decommission them with automation when not required to improve\n      overall performance efficiency.\n    \nImplementation steps\n\n\n\n            Identify which\n            accelerated\n            computing instances can address your requirements.\n          \n\n\n            For machine learning workloads, take advantage of\n            purpose-built hardware that is specific to your workload,\n            such\n            as\u00a0AWS             Trainium,\n            AWS             Inferentia, and\n            Amazon EC2 DL1. AWS Inferentia instances such as Inf2\n            instances\n            offer\n            up to 50% better performance/watt over comparable Amazon EC2\n            instances.\n          \n\n\n            Collect usage metrics for your accelerated computing\n            instances. For example, you can use CloudWatch agent to\n            collect metrics such as utilization_gpu and\n            utilization_memory for your GPUs as shown in\n            Collect\n            NVIDIA GPU metrics with Amazon CloudWatch.\n          \n\n\n            Optimize the code, network operation, and settings of\n            hardware accelerators to make sure that underlying hardware\n            is fully utilized.\n          \n\n\n\nOptimize\n                GPU settings\n\n\n\nGPU\n                Monitoring and Optimization in the Deep Learning\n                AMI\n\n\n\nOptimizing\n                I/O for GPU performance tuning of deep learning training\n                in Amazon SageMaker\n\n\n\n\n            Use the latest high performant libraries and GPU drivers.\n          \n\n\n            Use automation to release GPU instances when not in use.\n          \n\nResources\n\nRelated documents:\n\n\n\n\nGPU\n          instances\n\n\n\nInstances\n          with AWS Trainium\n\n\n\nInstances\n          with AWS Inferentia\n\n\n\nLet\u2019s\n          Architect! Architecting with custom chips and\n          accelerators\n\n\n\n\n\nAccelerated\n          Computing\n\n\n\nAmazon EC2 VT1 Instances\n\n\n\nHow\n          do I choose the appropriate Amazon EC2 instance type for my\n          workload?\n\n\n\nChoose\n          the best AI accelerator and model compilation for computer\n          vision inference with Amazon SageMaker\n\n\n\nRelated videos:\n\n\n\n\nHow\n          to select Amazon EC2 GPU instances for deep learning\n\n\n\nDeploying\n          Cost-Effective Deep Learning Inference\n\n\n Javascript is disabled or is unavailable in your browser.To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.Document ConventionsPERF02-BP05 Scale your compute resources dynamicallyData managementDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.", "metadata": {"source": "https://docs.aws.amazon.com/wellarchitected/latest/framework/perf_compute_hardware_compute_accelerators.html", "title": "PERF02-BP06 Use optimized hardware-based compute accelerators - AWS Well-Architected Framework", "description": "Use hardware accelerators to perform certain functions more efficiently than CPU-based alternatives.", "language": "en-US"}}